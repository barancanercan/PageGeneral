"""
PageGeneral - Division Extractor
LLM-based extraction of Turkish Infantry Divisions from paragraphs
Now using HuggingFace Transformers (local, no Ollama)
"""

import re
import json
from typing import List, Dict
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.llm import HFClient
import config


class DivisionExtractor:
    """Extract divisions from paragraphs using LLM + regex pre-filter"""

    def __init__(self, division_list: List[str]):
        """
        Args:
            division_list: List of division names to extract
                e.g., ["4. Piyade TÃ¼meni", "9. Piyade TÃ¼meni", ...]
        """
        self.division_list = division_list
        self.llm = HFClient()
        self.book_name = None
        self.book_id = None

        # Build regex pattern for pre-filtering
        self._build_regex_pattern()

    def _build_regex_pattern(self):
        """Build regex to pre-filter paragraphs containing division names"""
        # Escape special chars and build pattern
        patterns = []
        for div in self.division_list:
            # Handle Turkish characters
            escaped = re.escape(div)
            patterns.append(escaped)

        self.regex_pattern = '|'.join(patterns)

    def extract(self, paragraphs: List[Dict], verbose: bool = True) -> List[Dict]:
        """
        Extract divisions from paragraphs

        Args:
            paragraphs: List of {"text": str, "page": int} dicts
            verbose: Print progress

        Returns:
            List[{
                "id": str,
                "document": str,
                "metadata": {
                    "division": [str],
                    "confidence": float,
                    "source_page": int
                }
            }]
        """

        if verbose:
            print(f"\nğŸ” Pre-filtering {len(paragraphs)} paragraphs...")
            print(f"   (Regex â†’ LLM hybrid)")

        # Pre-filter: which paragraphs mention divisions?
        matching_indices = []
        for idx, para in enumerate(paragraphs):
            para_text = para["text"] if isinstance(para, dict) else para
            if re.search(self.regex_pattern, para_text, re.IGNORECASE):
                matching_indices.append(idx)

        if verbose:
            from tqdm import tqdm
            print(f"\n   Found {len(matching_indices)} matching paragraphs")
            pbar = tqdm(total=len(matching_indices))

        results = []

        # Process only matching paragraphs
        for idx in matching_indices:
            para = paragraphs[idx]
            para_text = para["text"] if isinstance(para, dict) else para
            para_page = para.get("page", idx) if isinstance(para, dict) else idx

            # Extract divisions using LLM
            extraction = self._extract_divisions(para_text)

            result = {
                "id": f"parag_{idx}",
                "embedding": [],  # Placeholder for vector embeddings
                "document": para_text,
                "metadata": {
                    "division": extraction.get("divisions", []),
                    "confidence": extraction.get("confidence", 0),
                    "source_page": para_page
                }
            }

            results.append(result)

            if verbose:
                pbar.update(1)

        if verbose:
            pbar.close()

        return results

    def _extract_divisions(self, paragraph_text: str) -> Dict:
        """
        Use LLM to extract which divisions are in this paragraph

        Returns: {
            "divisions": [str],
            "confidence": float
        }
        """

        # Build prompt
        divisions_str = ", ".join(self.division_list)

        prompt = f"""Verilen paragrafta aÅŸaÄŸÄ±daki tÃ¼menlerin hangilerinden bahsediliyor?

TÃ¼menler: {divisions_str}

PARAGRAF:
{paragraph_text}

KURALLAR:
1. Sadece listede olan tÃ¼menleri dÃ¶ndÃ¼r
2. Confidence deÄŸerini ÅŸu kriterlere gÃ¶re belirle:
   - 1.0: TÃ¼men adÄ± aÃ§Ä±kÃ§a ve tam olarak geÃ§iyor
   - 0.8-0.9: TÃ¼men numarasÄ± geÃ§iyor ama format biraz farklÄ±
   - 0.6-0.7: DolaylÄ± referans veya belirsiz ifade
   - 0.0: HiÃ§ tÃ¼men bulunamadÄ±

Sadece JSON dÃ¶ndÃ¼r:
{{"divisions": ["bulunan tÃ¼menler"], "confidence": 0.0-1.0 arasÄ± deÄŸer}}"""

        # LLM'den cevap al
        response = self.llm.generate(prompt)

        if not response:
            return {"divisions": [], "confidence": 0}

        # Parse JSON (with fallback)
        return self._parse_json_robust(response)

    def _parse_json_robust(self, response: str) -> Dict:
        """
        Robust JSON parsing with fallbacks
        Handles backticks, malformed JSON, etc.
        """
        response_clean = response.strip()

        # Remove markdown code blocks
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        elif response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]

        response_clean = response_clean.strip()

        # Try direct parse
        try:
            parsed = json.loads(response_clean)

            divisions = parsed.get("divisions", [])
            confidence = float(parsed.get("confidence", 0.5))

            # Ensure confidence is 0-1
            confidence = max(0, min(1, confidence))

            return {
                "divisions": divisions if isinstance(divisions, list) else [],
                "confidence": confidence
            }
        except json.JSONDecodeError:
            pass

        # Fallback: regex extract
        json_match = re.search(
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            response_clean,
            re.DOTALL
        )

        if json_match:
            try:
                parsed = json.loads(json_match.group())
                divisions = parsed.get("divisions", [])
                confidence = float(parsed.get("confidence", 0.5))
                confidence = max(0, min(1, confidence))

                return {
                    "divisions": divisions if isinstance(divisions, list) else [],
                    "confidence": confidence
                }
            except:
                pass

        # Last fallback: return empty
        return {"divisions": [], "confidence": 0}


# Test
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ§ª DivisionExtractor Test")
    print("=" * 70)

    # Test data
    divisions = config.DIVISION_LIST
    extractor = DivisionExtractor(divisions)

    test_paragraphs = [
        {"text": "5 nci Kafkas TÃ¼meni komutanÄ±, cepheye gitmek Ã¼zere hazÄ±rlanÄ±yordu.", "page": 1},
        {"text": "Hava Ã§ok soÄŸuktu ama askerler yÃ¼rÃ¼yÃ¼ÅŸteydi.", "page": 2},
        {"text": "24 ncÃ¼ TÃ¼men ile 36 ncÄ± TÃ¼men ortak operasyon yapacaklardÄ±.", "page": 3},
        {"text": "Hafif bir yaÄŸmur yaÄŸÄ±yordu.", "page": 4},
    ]

    print(f"\nğŸ“‹ Divisions: {len(divisions)} tÃ¼men")
    for div in divisions:
        print(f"   - {div}")

    print(f"\nğŸ” Testing {len(test_paragraphs)} paragraphs...")

    results = extractor.extract(test_paragraphs, verbose=True)

    print(f"\nğŸ“Š Results:")
    print("=" * 70)

    for result in results:
        print(f"\nğŸ“ Paragraf {result['id']}:")
        print(f"   Text: {result['document'][:80]}...")
        print(f"   TÃ¼menleri: {result['metadata']['division']}")
        print(f"   Sayfa: {result['metadata']['source_page']}")
        print(f"   Confidence: {result['metadata']['confidence']:.0%}")

    print("\nâœ… ALL TESTS PASSED!")
    print("=" * 70)