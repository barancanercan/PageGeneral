"""
PAGEGENERAL - RAG Pipeline
Ana sistem: PDF yÃ¼kle â†’ Metin iÅŸle â†’ LLM'ye sor â†’ Cevap dÃ¶n
Minimal MVP. Vector DB yok ÅŸimdilik.
"""

from pathlib import Path
from src.pdf_parser import PDFParser
from src.llm import OllamaClient
import config
import json
from datetime import datetime


class TextChunker:
    """Metni basit cÃ¼mlelere gÃ¶re chunks'a bÃ¶l"""

    @staticmethod
    def chunk(text: str, chunk_size: int = 512) -> list[dict]:
        """
        Metni chunk'lar (yaklaÅŸÄ±k 512 token)

        Args:
            text: TÃ¼m metin
            chunk_size: Chunk boyutu (token)

        Returns:
            List of chunks with metadata
        """
        # Basit: paragraflara gÃ¶re bÃ¶l
        paragraphs = text.split('\n\n')

        chunks = []
        current_chunk = ""
        chunk_id = 0

        for para in paragraphs:
            if not para.strip():
                continue

            # EÄŸer chunk dolu ise, kaydet
            if len(current_chunk) > chunk_size and current_chunk.strip():
                chunks.append({
                    "id": chunk_id,
                    "text": current_chunk.strip(),
                    "size": len(current_chunk)
                })
                chunk_id += 1
                current_chunk = ""

            current_chunk += para + "\n\n"

        # Son chunk'Ä± ekle
        if current_chunk.strip():
            chunks.append({
                "id": chunk_id,
                "text": current_chunk.strip(),
                "size": len(current_chunk)
            })

        return chunks


class RAGPipeline:
    """Ana RAG sistemi"""

    def __init__(self):
        self.parser = PDFParser()
        self.llm = OllamaClient()
        self.chunker = TextChunker()

    def ingest_pdf(self, pdf_path: str | Path) -> dict:
        """
        PDF'i yÃ¼kle ve iÅŸle

        Returns:
            {
                "status": "success" | "error",
                "chunks": metin chunks'larÄ±,
                "content": orijinal metin,
                "filename": dosya adÄ±
            }
        """
        pdf_path = Path(pdf_path)

        # PDF'i parse et
        parse_result = self.parser.parse(pdf_path)

        if parse_result['status'] != 'success':
            return {"status": "error", "error": parse_result.get('error')}

        content = parse_result['content']

        # Metni chunks'a bÃ¶l
        chunks = self.chunker.chunk(content, chunk_size=config.CHUNK_SIZE)

        if config.VERBOSE:
            print(f"âœ‚ï¸  {len(chunks)} chunk oluÅŸturuldu")

        return {
            "status": "success",
            "chunks": chunks,
            "content": content,
            "filename": parse_result['filename'],
            "pages": parse_result.get('pages', 0)
        }

    def query(self, question: str, context: str) -> dict:
        """
        Soru sor ve cevap al

        Args:
            question: KullanÄ±cÄ±nÄ±n sorusu
            context: BaÄŸlam (PDF'den Ã§Ä±kan metin)

        Returns:
            {
                "question": soru,
                "answer": cevap,
                "sources": kaynak chunks,
                "confidence": 0.0-1.0,
                "timestamp": zaman
            }
        """
        if config.VERBOSE:
            print(f"\nâ“ Sorgu: {question}")

        # Basit: context'in tamamÄ±nÄ± baÄŸlam olarak kullan
        # (Ä°leri aÅŸamada: semantic search yapacaÄŸÄ±z)

        prompt = f"""Verilen baÄŸlamdan hareketle, soruyu cevaplayÄ±nÄ±z.

BAÄLAM:
{context[:2000]}  # Ä°lk 2000 karakter

SORU:
{question}

CEVAPLARÄ±NÄ±z TÃœRKÃ‡E olmalÄ± ve sadece baÄŸlamdan bilgi kullanmalÄ±sÄ±nÄ±z.
EÄŸer baÄŸlamda cevap yoksa "Bu konuda verilen belgede bilgi bulunmamaktadÄ±r" deyin."""

        # LLM'den cevap al
        answer = self.llm.generate(prompt)

        if not answer:
            return {
                "status": "error",
                "error": "LLM sunucusu yanÄ±t vermedi. Ollama aÃ§Ä±k mÄ±?"
            }

        if config.VERBOSE:
            print(f"ğŸ’¬ Cevap alÄ±ndÄ± ({len(answer)} karakter)")

        return {
            "status": "success",
            "question": question,
            "answer": answer,
            "context_length": len(context),
            "timestamp": datetime.now().isoformat(),
            "confidence": 0.7  # Basit: sabit deÄŸer
        }

    def save_result(self, result: dict, output_file: Path = None):
        """Sonucu JSON'a kaydet"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = config.OUTPUT_DIR / f"result_{timestamp}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        if config.VERBOSE:
            print(f"ğŸ’¾ Kaydedildi: {output_file}")

        return output_file


def main():
    """Test"""

    pipeline = RAGPipeline()

    # PDF'leri yÃ¼kle
    pdf_files = list(config.INPUT_DIR.glob("*.pdf"))

    if not pdf_files:
        print(f"âš ï¸  {config.INPUT_DIR} klasÃ¶rÃ¼nde PDF yok")
        return

    print(f"ğŸ” {len(pdf_files)} PDF bulundu\n")

    # Ä°lk PDF'i yÃ¼kle
    pdf_file = pdf_files[0]
    print(f"ğŸ“¥ YÃ¼kleniyor: {pdf_file.name}")

    ingest_result = pipeline.ingest_pdf(pdf_file)

    if ingest_result['status'] != 'success':
        print(f"âŒ Hata: {ingest_result['error']}")
        return

    content = ingest_result['content']
    chunks = ingest_result['chunks']

    print(f"âœ… BaÅŸarÄ±lÄ±: {len(chunks)} chunk")

    # Test sorusu sor
    question = "Belgede ne anlatÄ±lÄ±yor?"

    print(f"\nâ“ Sorgu: {question}")

    query_result = pipeline.query(question, content)

    if query_result['status'] == 'success':
        print(f"\nğŸ’¬ Cevap:\n{query_result['answer']}")

        # Kaydet
        pipeline.save_result(query_result)
    else:
        print(f"âŒ {query_result['error']}")


if __name__ == "__main__":
    main()