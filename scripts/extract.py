#!/usr/bin/env python3
"""
PAGEGENERAL - Division Extraction Script
Turkish Military Division extraction from historical PDFs
Using HuggingFace Qwen2.5-7B-Instruct + LLM-based extraction
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
from datetime import datetime
from src.pdf_parser import PDFParser
from src.division_extractor import DivisionExtractor
import config


def main():
    print("\n" + "=" * 80)
    print("ğŸ–ï¸  PAGEGENERAL - Division Extraction")
    print("=" * 80)
    
    pdf_files = list(config.INPUT_DIR.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ {config.INPUT_DIR} klasÃ¶rÃ¼nde PDF bulunamadÄ±!")
        return
    
    print(f"\nğŸ“‚ {len(pdf_files)} PDF bulundu\n")
    
    divisions = config.DIVISION_LIST
    print(f"ğŸ“‹ {len(divisions)} tÃ¼men target:")
    for div in divisions:
        print(f"   - {div}")
    
    all_results = []
    
    for pdf_file in pdf_files:
        print(f"\nğŸ“¥ YÃ¼kleniyor: {pdf_file.name}")
        
        parser = PDFParser()
        parse_result = parser.parse(pdf_file)
        
        if parse_result['status'] != 'success':
            print(f"âŒ Parse hatasÄ±: {parse_result.get('error')}")
            continue
        
        content = parse_result['content']
        
        paragraphs = content.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        print(f"âœ‚ï¸  {len(paragraphs)} paragraf oluÅŸturuldu")
        
        print(f"\nğŸ” Extraction baÅŸlÄ±yor...")
        extractor = DivisionExtractor(divisions)
        results = extractor.extract(paragraphs, verbose=True)
        
        for result in results:
            result['book'] = pdf_file.stem
            result['timestamp'] = datetime.now().isoformat()
        
        all_results.extend(results)
        
        print(f"\nâœ… {len(results)} paragraf iÅŸlendi")
    
    if all_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = config.OUTPUT_DIR / f"extractions_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Kaydedildi: {output_file}")
        print(f"   ({len(all_results)} extraction)")
        
        print("\nğŸ“Š SUMMARY:")
        print("=" * 80)
        
        unique_divisions = set()
        for result in all_results:
            unique_divisions.update(result.get('divisions', []))
        
        print(f"âœ… Toplam extraction: {len(all_results)}")
        print(f"âœ… Benzersiz tÃ¼men: {len(unique_divisions)}")
        print(f"âœ… Ã‡Ä±ktÄ±: {output_file}")
        
        if unique_divisions:
            print(f"\nğŸ–ï¸  BulunmuÅŸ tÃ¼menleri:")
            for div in sorted(unique_divisions):
                count = sum(1 for r in all_results if div in r.get('divisions', []))
                print(f"   - {div}: {count} paragraf")
    else:
        print("\nâš ï¸  HiÃ§ extraction bulunamadÄ±")
    
    print("\n" + "=" * 80)
    print("âœ… Ä°ÅŸlem tamamlandÄ±!")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
