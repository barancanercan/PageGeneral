#!/usr/bin/env python3
"""
PAGEGENERAL - Quick Test (Sadece 100 paragraf)
HÄ±zlÄ± test iÃ§in ilk 100 paragrafa sÄ±nÄ±rla
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.rag_pipeline import RAGPipeline
    from src.query_engine import QueryEngine
    from src.llm import OllamaClient
    import config
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from rag_pipeline import RAGPipeline
    from query_engine import QueryEngine
    from llm import OllamaClient
    sys.path.insert(0, str(project_root))
    import config

import json


def print_header():
    """BaÅŸlÄ±k gÃ¶ster"""
    print("\n" + "=" * 70)
    print("ğŸ–ï¸  PAGEGENERAL - QUICK TEST (100 Paragraf)")
    print("=" * 70)
    print("HÄ±zlÄ± test iÃ§in ilk 100 paragraf")
    print("=" * 70 + "\n")


def print_separator(title=""):
    """AyÄ±rÄ±cÄ± gÃ¶ster"""
    if title:
        print(f"\n{title}")
    print("-" * 70)


def main():
    """Ana test"""

    print_header()

    # Ollama kontrol
    llm = OllamaClient()
    if not llm.is_available():
        print("âŒ Ollama aÃ§Ä±k deÄŸil!")
        return

    print(f"âœ… Ollama baÄŸlÄ±: {config.OLLAMA_BASE_URL}\n")

    # RAG pipeline
    book_name = "TÃ¼rk Ä°stiklal Harbi - Mondros MÃ¼tarekesi (TEST)"
    book_id = "turk_istiklal_harbi_mondros_test"
    pipeline = RAGPipeline(book_name, book_id)

    # PDF bul
    pdf_files = list(config.INPUT_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"âŒ PDF bulunamadÄ±!")
        return

    pdf_file = pdf_files[0]
    print(f"ğŸ“¥ YÃ¼kleniyor: {pdf_file.name}")

    # Parse PDF
    parse_result = pipeline.parser.parse(pdf_file)
    if parse_result['status'] != 'success':
        print(f"âŒ Parse hatasÄ±: {parse_result['error']}")
        return

    content = parse_result['content']

    # Paragraf bÃ¶l
    paragraphs = content.split('\n\n')
    paragraphs = [p.strip() for p in paragraphs if p.strip()]

    # âš ï¸ Ä°LK 100 PARAGRAFI AL
    paragraphs_test = paragraphs[:100]

    print(f"âœ‚ï¸  {len(paragraphs_test)} paragraf iÅŸlenecek (100 limit)\n")

    # LLM extraction
    print(f"ğŸ¤– Division Extraction (100 para)...\n")
    extraction_results = pipeline.extractor.extract(paragraphs_test, verbose=True)

    # Chunks
    print(f"\nğŸ“¦ Chunks oluÅŸturuluyor...")
    chunks = pipeline.chunker.create_chunks(extraction_results)
    print(f"âœ‚ï¸  {len(chunks)} chunk oluÅŸturuldu")

    if not chunks:
        print("âŒ HiÃ§ chunk yok!")
        return

    # Chromadb ingestion
    print(f"\nğŸ”— Chromadb'ye yÃ¼kleniyor...")
    pipeline.vs.ingest_chunks(chunks)

    # Ä°statistikler
    divisions = set()
    for chunk in chunks:
        division_str = chunk["metadata"]["division"]
        if division_str:
            for div in division_str.split(","):
                div_clean = div.strip()
                if div_clean:
                    divisions.add(div_clean)

    print_separator("ğŸ“ BULUNAN TÃœMENLERI:")
    for i, div in enumerate(sorted(divisions), 1):
        print(f"  {i}. {div}")

    print(f"\nğŸ“Š Ä°statistikler:")
    print(f"  - Test Paragraf: {len(paragraphs_test)}")
    print(f"  - Chunks: {len(chunks)}")
    print(f"  - Divisions: {len(divisions)}")

    # Query Engine test
    query_engine = QueryEngine()

    print_separator("ğŸ’¬ QUERY TEST...")

    test_questions = [
        ("4. Piyade TÃ¼meni", "Bu tÃ¼men nerede savaÅŸtÄ±?"),
        ("9. Piyade TÃ¼meni", "TÃ¼menin komutanÄ± kimdi?"),
    ]

    for division, question in test_questions:
        if division not in divisions:
            print(f"\nâš ï¸  {division} bulunamadÄ±, skip...")
            continue

        print(f"\nâ“ Sorgu: {question}")
        print(f"ğŸ“ Division: {division}")

        result = query_engine.generate_answer_with_sources(question, division, top_k=3)

        print(f"\nğŸ’¬ Cevap:\n{result['answer']}\n")

        print(f"ğŸ“ Kaynaklar ({len(result['sources'])}):")
        for src in result['sources']:
            page = src['metadata'].get('source_page')
            print(f"  - {src['id']}: s.{page}")

    print_separator()
    print("âœ… QUICK TEST TAMAMLANDI!\n")


if __name__ == "__main__":
    main()
